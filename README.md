“Computer graphics is the study of computational processes involving geometric models and digital images” [1]. The evolution of 3D graphics engines has been extensively documented in the literature, tracing their development from early rasterization-based approaches to modern techniques incorporating ray tracing and real-time rendering. Work by Hughes(2014)[2] provide comprehensive insights into the fundamental principles and algorithms underpinning 3D graphics engines, highlighting advancements in rendering, shading, and lighting techniques.
Real-time rendering is a critical aspect of 3D graphics engines, enabling the generation of immersive virtual environments at interactive frame rates. Works by Akenine-Möller et al. (2008)[3] delve into the intricacies of real-time rendering, discussing topics such as visibility determination, geometric transformations, and rasterization algorithms. These studies offer valuable insights into the computational challenges and optimization strategies involved in achieving high-performance rendering.
The Lightweight Java Game Library (LWJGL) has emerged as a popular framework for developing 3D graphics applications in Java. Work by Kessenich, Sellers and Shreiner(2016)[4] explores the capabilities of LWJGL for game development, emphasizing its support for OpenGL bindings and low-level access to hardware-accelerated graphics functionality. Additionally, studies by Lübke et al. (2004) and Fry et al. (2003) discuss the advantages of using Java for graphics programming, highlighting its portability, ease of development, and suitability for educational purposes.
The problem that 3D graphics engines solve is that 3D graphics engines enable the development of interactive experiences across various platforms, from video games to architectural visualizations and simulations. These engines empower developers to create dynamic environments where users can explore, interact, and engage with virtual worlds. This is important because the gaming industry heavily relies on 3D graphics engines to create visually stunning and immersive gaming experiences. These engines contribute to the cultural landscape by offering new forms of entertainment and storytelling. Beyond entertainment, 3D graphics engines play a crucial role in education and training simulations. From medical training to flight simulations, these tools provide realistic environments for learning and skill development. Also, Architects, engineers, and designers use 3D graphics engines to visualize and prototype their ideas. These tools facilitate the design process by allowing professionals to explore different concepts and iterate on their designs in a virtual space. 
A legal and Intellectual property issue is that developers using 3D graphics software must adhere to copyright and licensing agreements to avoid legal disputes over intellectual property rights. Additionally, respecting the intellectual property of others ensures a fair and ethical development environment.
Project Aims:
The primary aim of the project is to design and implement a robust 3D graphics engine using LWJGL capable of rendering virtual environments with high visual fidelity and real-time interactivity. The project aims to incorporate rendering techniques such as shader-based rendering, dynamic lighting and shadow mapping to enhance the realism and visual quality of rendered scenes.
The project objectives include developing the core components of the rendering pipeline, including vertex and fragment shaders, rasterization algorithms, and texture mapping techniques, using LWJGL's OpenGL bindings. Implement functionality for loading and rendering 3D mesh models from external file formats. Incorporate various lighting types and shadows to accurately simulate the interaction of light with surfaces in the rendered scene. Develop mechanisms for user interaction, including camera controls and input handling (keyboard, mouse) allowing users to navigate and interact with the 3D scene in real-time. 
Technical Documentation:
Rendering pipeline:
The rendering pipeline is a series of steps that a 3D graphics engine follows to transform 3D models and scenes into 2D images that can be displayed on a screen. The rendering starts taking as its input a list of vertices in the form of Vertex Buffers. Those vertices are processed by a vertex shader whose main purpose is to calculate the projected position of each vertex into the screen space. This shader can generate also other outputs related to colour or texture, but its main goal is to project the vertices into the screen space. The geometry processing stage connects the vertices that are transformed by the vertex shader to form triangles. The rasterization stage takes the triangles generated in the previous stages, clips them and transforms them into pixel-sized fragments. Those fragments are used during the fragment processing stage by the fragment shader to generate pixels assigning them the final colour that gets written into the framebuffer. The framebuffer is the final result of the graphics pipeline. It holds the value of each pixel that should be drawn to the screen.[5]
Vertices: 
A VBO is just a memory buffer stored in the graphics card memory that stores vertices. They can hold not just coordinates but other information, such as textures, colour, etc. A Vertex Array Objects (VAOs) is an object that contains one or more VBOs which are usually called attribute lists. Each attribute list can hold one type of data: position, colour, texture, etc. [6] A VAO is like a wrapper that groups a set of definitions for the data that is going to be stored in the graphics card. When a VAO is created we get an identifier. We use that identifier to render it and the elements it contains using the definitions we specified during its creation.
The first thing that we do is to create the VAO by calling the glGenVertexArrays function and bind it by calling the glBindVertexArray function. After that, we need to create the VBO by calling the glGenBuffers and put the data into it. The first thing that we do is to create the VAO by calling the glGenVertexArrays function and bind it by calling the glBindVertexArray function. After that, we need to create the VBO by calling the glGenBuffers and put the data into it. This is done using glVertexAttribPointer. This is repeated for all VBOs in the VAO
Shader Handling:
The ShaderProgram class receives the source code for the shader modules (vertex and fragment) and compiles them and then links them together into a shader program.[7] First an OpenGL program is created using glCreateProgram() this creates a ID to identify the program with. Then for each shader is loaded and for each shader a new module is created with the specified type. The shader is complied using glCompileShader(shaderID) and then attached to the program using glAttachShader(programID, shaderID). The linking is performed by glLinkProgram(programID) Once the shader program has been linked, the compiled vertex and fragment shaders can be freed up by calling glDetachShader.
Projection Matrix:
The perspective projection matrix handles aspect ratio of our drawing area so objects won’t be distorted. It also will handle the distance so objects far away from the camera will be drawn smaller. The projection matrix will also consider our field of view and the maximum distance to be displayed.[8]
The matrix is handled in the vertex shader as a Uniform. Uniforms are global GLSL variables that shaders can use and that we will employ to pass data that is common to all elements or to a model. The output position of the vertices are calculated by multiplying them by the projection matrix.
Models and Entities:
A model is a structure which glues together vertices, colours, textures and materials. A model may be composed of several meshes and can be used by several entities. An entity represents anything that is part of the 3D scene. An entity has specific data such a position, which is needed when it comes rendering the object. The render process is started by getting the models and then draw the entities associated to that model. This is because efficiency, since several entities can share the same model is better to set up the elements that belong to the model once and later on handle the data that is specific for each entity.[9]
A model stores a list of Mesh instances and has a unique identifier. In addition to that, the list of entities that are associated to that model are stored as well.
A Model instance also has a unique identifier and defines attributes for its position as a 3 components vector, its scale, a float and rotation as a quaternion. All the transformations applied to a model are defined by a 4x4 matrix, therefore a Model instance stores a Matrix4f instance which is automatically constructed by JOML method translationRotateScale using position, scale and rotation.
Texture Loading:
A texture is an image which is mapped to a model to set the colour of the pixels of the model. What you do is assign points in the image texture to the vertices in your model. With that information OpenGL is able to calculate the colour to apply to the other pixels based on the texture image.[10]
The texture image does not have to be the same size as the model. It can be larger or smaller. OpenGL will extrapolate the colour if the pixel to be processed cannot be mapped to a specific point in the texture. In order to apply a texture to a model, texture coordinates are assigned to each of our vertices. In the texture coordinate system, the origin is in the top left corner of the image and the maximum value of the x or y value is 1. Texture Coordinates are related to position coordinates using a VBO.
The texture image is loaded using stbi_load and is stored in a ByteBuffer. A texture identifier is created using glGenTextures and then bound using glBindTexture. OpenGL this then told how to the RGBA bytes by passing GL_UNPACK_ALIGNMENT into glPixelStorei. The texture data is loaded calling glTexImage2D. The glTexParameteri function is called tell OpenGl that when a pixel is drawn with no direct one to one association to a texture coordinate it will pick the nearest texture coordinate point. After that, we generate a mipmap. A mipmap is a decreasing resolution set of images generated from a high detailed texture. These lower resolution images will be used automatically when our object is scaled.[11] We do this when calling the glGenerateMipmap function.
It is very frequent that models reuse the same texture, therefore, instead of loading the same texture multiple times, so textures already loaded are cached to load each texture just once. This is controlled by the TextureCache class which hold a map of textures.
Model loading from file:
The loadModel method has an argument named flags. This parameter allows to tune the loading process. It is invoked by the other loadModel methods they pass some values that are useful in most of the situations:
•	aiProcess_JoinIdenticalVertices: This flag reduces the number of vertices that are used, identifying those that can be reused between faces.
•	aiProcess_Triangulate: The model may use quads or other geometries to define their elements.
•	aiProcess_FixInfacingNormals: This flag try to reverse normals that may point inwards.
•	aiProcess_CalcTangentSpace: This flag calculates tangent and bitangents using normals information.
•	aiProcess_LimitBoneWeights: This flag limits the number of weights that affect a single vertex.
•	aiProcess_PreTransformVertices: This flag performs some transformation over the data loaded so the model is placed in the origin and the coordinates are corrected to math OpenGL coordinate System.
When loading a model, first get the material colour. After that, the material is checked if it defines a texture or not. If there is a texture path, store the texture path and delegate texture creation to the TextureCache class.
Next the mesh is processed by processing the mesh elements (vertices, texture coordinates, etc.). This is done by using a method that converts the mesh data into an array. Then a Mesh object is created.
The resulting mesh is then rendered as described previously.
Back Face Culling:
The reduce the amount of data that is being rendered by applying face culling. As an example, a cube is made of six faces but faces that are not visible are still being rendered. Faces that cannot be seen should be discarded immediately and this is what face culling does. For a cube you can only see 3 faces at the same time, so half of the faces can be discarded just by applying face culling (this will only be valid if your game does not require you to dive into the inner side of a model). For every triangle, face culling checks if it's facing towards us and discards the ones that are not facing that direction.
This is implemented using glEnable(GL_CULL_FACE) and glCullFace(GL_BACK) in the constructor of the render class.
Light:
There are four different light types used in this project:
•	Point light: This type of light models a light source that’s emitted uniformly from a point in space in all directions.
•	Spot light: This type of light models a light source that’s emitted from a point in space, but instead of emitting in all directions is restricted to a cone.
•	Directional light: This type for light models the light that we receive from the sun, all the objects in the 3D the space are hit by parallel ray lights coming from a specific direction. No matter if the object is close or far away, all the ray lights impact the objects with the same angle.
•	Ambient light: This type of light comes from everywhere in the space and illuminates all the objects in the same way.
The Phong shading algorithm will model the effects of light for each point in our model, that is for every vertex. This is why it’s called a local illumination simulation, and this is the reason why this algorithm will not calculate shadows: it will just calculate the light to be applied to every vertex without taking into consideration if the vertex is behind an object that blocks the light. But, because of that, it's a simple and fast algorithm.[12]
The Phong algorithm considers three components for lighting:
•	Ambient light: models light that comes from everywhere, this will serve us to illuminate (with the required intensity) the areas that are not hit by any light, it’s like a background light.
•	Diffuse reflectance: takes into consideration that surfaces that are facing the light source are brighter.
•	Specular reflectance: models how light reflects on polished or metallic surfaces.
At the end what we want to obtain is a factor that, multiplied by colour assigned to a fragment, will set that colour brighter or darker depending on the light it receives. Let’s name our components as A for ambient, D for diffuse and S for specular. That factor will be the addition of those components. Those components are colours, that is the colour components that each light component contributes to. This is due to the fact that light components will not only provide a degree of intensity, but they can modify the colour of the model. In the fragment shader, the original fragment colour is multiplied by that light colour.
Normal of a plane is a vector perpendicular to that plane which has a length equal to one. A plane can have two normals. Normals in 3D graphics are used for lighting, so the normal which is oriented towards the source of light is used. This is usually the normal on the exterior face of the model.
Diffuse reflectance models the fact that surfaces which face in a perpendicular way to the light source look brighter than surfaces where light is received in a more indirect angle. This is calculated using normal. Instead of drawing ray coming from the source of light, vectors are drawn from each point to the point of light. The light intensity is scale based on the angle between the normal and the vector, with smaller angles denoting greater light intensity.[12]
Specular component models how light is diffused more when reflecting off of rough surfaces and diffused less when reflecting off of smooth surfaces. To calculate this, take the vector that was pointed towards the light source and reverse it and calculate the reflected light using the reflect function in the shader. But to be able to see the reflected light the camera must be in the correct direction. This is done by calculating the vector between the camera and the point where the light is reflecting off of. [12]
Spot light contribution is calculated in the same way as a point light with some exceptions. The points for which the vector that points from the vertex position to the light source is not contained inside the light cone are not affected by the point light.
Normal Mapping:
.By changing the normals for each fragment of the surface, surface imperfections can be modelled to render them in a more realistic way. This is done by loading another texture that stores the normals for the surface. Each pixel of the normal texture will contain the values of the x, y, z coordinates of the normal stored as an RGB value. The dominant colour of normal maps tends to blue. This is due to the fact that normals point to the positive z axis. The z component will usually have a much higher value than the x and y ones for plain surfaces as the normal points out of the surface. Since x, y, z coordinates are mapped to RGB, the blue component will have also a higher value.[13] 
Usually, normal maps are not defined in that way, they usually are defined in tangent space. The tangent space is a coordinate system that is local to each triangle of the model. In that coordinate space the z axis always points out of the surface. In order to handle tangent space, normals, tangent and bi-tangent vectors are required. the tangent and bitangent vectors are perpendicular vectors to the normal one. These vectors are required to calculate the TBN matrix which will allow for the data that is in tangent space to be used with the coordinate system we are using in the shaders.
If a texture has a normal map applied to it the fragment shader uses the tangent and bitangent data with the normal data to calculate the TBN matrix. Then the normal value from the normal map texture and the TBN Matrix are used to pass from tangent space to view space.
Fog:
Fog is defined by two attributes, its colour and its density. To apply the fog effect, 3D scene objects fade into the fog colour as long as they get far away from the camera. This is controlled by fogFactor, and its range is from 0 to 1. When the fogFactor is 1, it means that the object will not be affected by fog as it’s a nearby object. When the fogFactor takes the 0 value, it means that the objects will be completely hidden in the fog. The equation for this is:
finalColour  = (1-fogFactor) * fogColour  + fogFactor * fragmentColour
and fogFactor is:
fogFactor = e^-(distance * fogDensity)^2
Skeletal Animation:
In skeletal animation the way a model animates is defined by its underlying skeleton. A skeleton is defined by a hierarchy of special elements called bones. These bones are defined by their position and rotation. The hierarchy means that the final position for each bones is affected by the position of their parents. For instance, the position of a wrist is modified if a character moves the elbow and also if it moves the shoulder. In addition to bones we still have vertices, the points that define the triangles that compose a 3D model. But in skeletal animation, vertices are drawn based on the position of the bones they relate to.
An animated model defines in essence the following additional information:
•	A tree like structure, composed by bones, which define a hierarchy where we can compose transformations.
•	
•	Each mesh, besides containing information about vertex position, normals, etc, will include information about which bones does this vertex relate to (by using a bone index) and how much they are affected (that is modulating the effect by using a weight factor).
•	A set of animation key frames which define the specific transformations that should be applied to each bone and by extension will modify the associated vertices. A model can define several animations and each of them may be composed of several animation key frames. With an animation the key frames are iterated over, and we can even interoperate between them. In essence, for a specific instant of time we are applying to each vertex the transformations associated to the related bones.[14]
Each bone is defined by the following attributes:
•	A name.
•	An offset matrix: This used to compute the final transformations that should be used by each bone.
Bones also point to a list of weights. Each weight is defined by the following attributes:
•	A weight factor -  the number that will be used to modulate the influence of the bone’s transformation associated to each vertex.
•	A vertex identifier - the vertex associated to the current bone.
Each vertex, besides containing position, normals and texture coordinates will have now a set of indices (typically four values) of the bones that affect those vertices (jointIndices) and a set of weights that will modulate that effect. Each vertex will be modified according to the transformation matrices associated to each joint in order to calculate final position.
Assimp scene object defines a Node’s hierarchy. Each Node is defined by a name a list of children node. Animations use these nodes to define the transformations that should be applied. Every bone is a node, and has a parent, except the root node, and possible a set of children. There are special nodes that are not bones, they are used to group transformations, and should be handled when calculating the transformations.
A scene also defines a set of animations. A single model can have more than one animation to model how a character walks, runs, etc. Each of these animations define different transformations. An animation has the following attributes:
•	A name.
•	A duration. That is, the duration in time of the animation. The name may seem confusing since an animation is the list of transformations that should be applied to each node for each different frame.
•	A list of animation channels. An animation channel contains, for a specific instant in time the translation, rotation and scaling information that should be applied to each node. The class that models the data contained in the animation channels is the AINodeAnim.
For a specific instant of time, for a frame, the transformation to be applied to a bone is the transformation defined in the animation channel for that instant, multiplied by the transformations of all the parent nodes up to the root node. To extract the animation data, the process is as follows:
•	Construct the node hierarchy.
•	For each animation, iterate over each animation channel (for each animation node) and construct the transformation matrices for each of the bones for all the potential animation frames. Those transformation matrices are a combination of the transformation matrix of the node associated to the bone and the bone transformation matrices.
•	We start at the root node, and for each frame, build transformation matrix for that node, which is the transformation matrix of the node multiplied by the composition of the translation, rotation and scale matrix of that specific frame for that node.
•	We then get the bones associated to that node and complement that transformation by multiplying the offset matrices of the bones. The result will be a transformation matrix associated to the related bones for that specific frame, which will be used in the shaders.
•	After that, we iterate over the children nodes, passing the transformation matrix of the parent node to be used also in combination with the children node transformations.
When loading an animated model the aiProcess_PreTransformVertices flag cannot be used this is because it removes animation data information.
While processing the meshes the associated bones and weights for each vertex are also processed. While we are processing them, the processBones method traverses the bone definition for a specific mesh, getting their weights and generating filling up three lists:
•	boneList: It contains a list of bones, with their offset matrices. 
•	boneIds: It contains the identifiers of the bones for each vertex of the Mesh. Bones are identified by its position when rendering. This list only contains the bones for a specific Mesh.
•	weights: It contains the weights for each vertex of the Mesh to be applied for the associated bones.
The information retrieved in this method is encapsulated in the AnimMeshData record (defined inside the ModelLoader class). 
The processAnimations method returns a List of Model.Animation instances. For each of these animations, a list of animation frames is constructed. (Model.AnimatedFrame instances), which are a list of the transformation matrices to be applied to each of the bones that compose the model. For each of the animations, the maximum number of frames is calculated by calling the method calcAnimationMaxFrames. The animation data is stored in the Model class.
The buildFrameMatrices method iterates over the different frames and builds the transformation matrices for the bones. For each frame it starts with the root node, and will apply the transformations recursively from top to down of the nodes hierarchy.
After getting the transformation associated to the node, check if this node has an animation node associated to it. If so, the proper translation, rotation and scaling transformations that apply to the frame that we are handling, are needed. With that information, it can get the bones associated to that node and update the transformation matrix for each of those bones, for that specific frame by multiplying:
•	The model inverse global transformation matrix (the inverse of the root node transformation matrix).
•	The transformation matrix for the node.
•	The bone offset matrix.
Cascade Shadow Maps:
How to check in an efficient manner if a ray can be cast without collisions? A light source can theoretically cast infinitely ray lights, so how do we check if a ray light is blocked or not? What can be done instead of casting ray lights is to look at the 3D scene from the light’s perspective and render the scene from that location. We can set the camera at the light position and render the scene so we can store the depth for each fragment. This is equivalent to calculate the distance of each fragment to the light source. At the end, what we are doing is storing the minimum distance as seen from the light source as a shadow map.
With that information a 3D scene can be rendered as usual and check the distance for each fragment to the light source with the minimum stored distance. If the distance is less that the value stored in the shadow map, then the object is in light, otherwise it's in shadow. There can be several objects that could be hit by the same ray light, but the minimum distance stored.
Thus, shadow mapping is a two -step process:
First, render the scene from the light space into a shadow map to get the minimum distances.
Second, render the scene from the camera point of view and use that depth map to calculate if objects are in shadow or not.
In order to render the depth map the depth buffer is needed. When rendering a scene, all the depth information is stored in a buffer named, the depth-buffer (or z-buffer). That depth information is the z value of each of the fragment that is rendered. If an object is more distant than another, we must calculate how this affects their x and y coordinates through the perspective projection matrix. This is not calculated automatically depending on the z value. What is stored in the z coordinate is the depth of that fragment.
The solution presented above, as it is, does not produce quality results for open spaces. The reason for that is that shadows resolution is limited by the texture size. We are covering now a potentially huge area, and textures we are using to store depth information have not enough resolution in order to get good results. increase texture resolution is not sufficient to completely fix the problem as huge textures would be required for that. 
Cascaded Shadow Maps (CSM) which are an improvement over the plain shadow maps.
The key concept is that, shadows of objects that are closer to the camera need to have a higher quality than shadows for distant objects. One approach could be to just render shadows for objects close to the camera, but this would cause shadows to appear / disappear as long as we move through the scene.
The approach that Cascaded Shadow Maps use is to divide the view frustum into several splits. Splits closer to the camera cover a smaller amount space whilst distant regions cover a much wider region of space.
For each of these splits, the depth map is rendered, adjusting the light view and projection matrices to cover fit to each split. Thus, the texture that stores the depth map covers a reduced area of the view frustum. And, since the split closest to the camera covers less space, the depth resolution is increased.[15]
As many depth textures as splits, and we will also change the light view and projection matrices for each. Hence, the steps to be done in order to apply CSMs are:
•	Divide the view frustum into n splits.
•	While rendering the depth map, for each split:
o	Calculate light view and projection matrices.
o	Render the scene from light’s perspective into a separate depth map
•	While rendering the scene:
o	Use the depths maps calculated above.
o	Determine the split that the fragment to be drawn belongs to.
o	Calculate shadow factor as in shadow maps.
After Splitting the projection matrix, retrieve the matrices that are needed to calculate the splits data, the view and projection matrices, the light position and the near and far clips of the perspective projection being used to render the scene. With that information the split distances for each of the shadow cascades can be calculated.
The algorithm used to calculate the split positions, uses a logarithm schema to better distribute the distances. The advantage of the logarithm schema is that it uses less space for near view splits, achieving a higher resolution for the elements closer to the camera.
Then a loop is used to calculate all the data for the cascade splits. In that loop, the frustum corners are created in Normalized Device Coordinates space. After that, those coordinates are projected into world space by using the inverse of the view and perspective matrices.
At this point, frustumCorners variable has the coordinates of a cube which contains the visible space, but the world coordinates for this specific cascade split are needed. Therefore, the next step is to use the cascade distances calculated at the beginning of the method. The coordinates of near and far planes are adjusted for this specific split according to the pre-calculated distances. With that information, we can now calculate the view matrix.
The ShadowBuffer class defines two constants that determine the size of the texture that will hold the depth map. It also defines two attributes, one for the Frame Buffers and one for the texture. In the constructor, a new Frame Buffers and an array of textures is created. Each element of that array will be used to render a shadow map for each cascade shadow split. For the Frame Buffers we will use as the pixel format the constant GL_DEPTH_COMPONENT since we are only interested in storing depth values. Then we attach the Frame Buffers to the texture instance.
